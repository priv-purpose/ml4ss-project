{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "from Components import Comment, Post, POSPost, POSComment\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2 = [], []\n",
    "for i in range(6):\n",
    "    d1 = pickle.load(open(\"data/v1-{}\".format(i), \"rb\"))\n",
    "    d2 = pickle.load(open(\"data/v2-{}\".format(i), \"rb\"))\n",
    "    v1.append(d1)\n",
    "    v2.append(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_counter = 0\n",
    "full_list = []\n",
    "for i in range(6):\n",
    "    for j, p in enumerate(v1[i]):\n",
    "        full_list.append((post_counter, p, v2[i][j]))\n",
    "        post_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18570"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(tup):\n",
    "    # returns True if POS Tag is one of allowed\n",
    "    allowed_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    return tup[1] in allowed_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18570/18570 [00:00<00:00, 36754.92it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_by_post = []\n",
    "idf_counter = defaultdict(int) # counts the number of documents a term appears in\n",
    "for p_idx, p, pos_p in tqdm(full_list):\n",
    "    filtered_pos = list(filter(filter_func, pos_p.body_pos))\n",
    "    word_counter = defaultdict(int)\n",
    "    for word, tag in filtered_pos:\n",
    "        if word_counter[word] == 0:\n",
    "            idf_counter[word] += 1\n",
    "        word_counter[word] += 1\n",
    "    \n",
    "    tf_by_post.append(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_term_number = 5000\n",
    "all_used_terms = list(sorted(idf_counter.keys(), key=lambda x: idf_counter[x], reverse=True))[:top_term_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of terms: 5000\n",
      "number of documents: 18570\n"
     ]
    }
   ],
   "source": [
    "num_terms = len(all_used_terms)\n",
    "num_docs = post_counter\n",
    "print('number of terms: %d' % num_terms)\n",
    "print('number of documents: %d' % num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18570/18570 [01:54<00:00, 161.57it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectors = []\n",
    "for p_idx, p, pos_p in tqdm(full_list):\n",
    "    post_vector = np.zeros((num_terms,))\n",
    "    for t_idx, term in enumerate(all_used_terms):\n",
    "        tf_val = tf_by_post[p_idx][term]\n",
    "        idf_val = np.log(num_docs/idf_counter[term])\n",
    "        post_vector[t_idx] = tf_val * idf_val\n",
    "    tfidf_vectors.append(post_vector[:])\n",
    "    del post_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist(vec1, vec2):\n",
    "    size1 = np.linalg.norm(vec1)\n",
    "    size2 = np.linalg.norm(vec2)\n",
    "    inner_prod = np.sum(vec1 * vec2)\n",
    "    if (size1 * size2) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return inner_prod / (size1 * size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18570/18570 [00:00<00:00, 39917.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# tfidf search implementation\n",
    "search_idx = 10000\n",
    "search_vec = tfidf_vectors[search_idx]\n",
    "sim_info = []\n",
    "for p_idx, p, pos_p in tqdm(full_list):\n",
    "    if p_idx == search_idx: \n",
    "        continue\n",
    "    dist_from_query = cos_dist(search_vec, tfidf_vectors[p_idx])\n",
    "    sim_info.append((p_idx, dist_from_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Org post-----\n",
      "[중앙도서관 열람실 사용할때 자리배정좀...]\n",
      "카이스트 학우분들! 중앙도서관 열람실 이용할땐 꼭 자리배정 후에 사용해주세요\n",
      "키오스크까지 가는게 귀찮다면 KAIST Library 앱으로 간단히 자리배정도 가능하니 꼭 해주세요\n",
      "빈 자리 배정 받고 가면 항상 사람이 차있어서 가끔은 기분 상하는 일이 꽤 있어요 ㅠㅠ 간단한 매너니 지켜주시면 감사하겠습니다\n",
      "----------\n",
      "#분실과_습득\n",
      "https://talkyou.in/pages/KaDaejeon/posts/16856/\n",
      "익명 아바타\n",
      "Org post over-----\n",
      "0.3300355692190233\n",
      "자리 다 쓰고 가실 때 시간이 남았으면 꼭 좌석배정취소 누르고 가주세요\n",
      "자리에 아무것도 없는데 230분 예약되어 있어서 띠용~하고 갑니다\n",
      "^^^^^^^^^^^^^^^^^\n",
      "0.32837372863941183\n",
      "안녕하세요. 다소 황당한 일이 있어 제보합니다. 오늘 오후에 도서관에서 공부하고 있다가 자리에 짐을 두고 밥을 먹으러 나갔는데, 돌아와 보니 어떤 분이 제 짐을 한쪽으로 밀어두고 의자에 걸어둔 가방은 바닥에 내려놓고 공부를 하고 계시더라고요. 평소에는 꼭 자리배정을 하고 앉는데 오늘은 이전에 쓰던 사람이 취소를 안 해서 두시간정도 남아있길래 그냥 앉았습니다. 시간이 지난 걸 모르고 30분정도 자리를 비웠다 돌아왔는데 그 사이에 다른 분이 앉은 것을 보니 좀 어이가 없었습니다. 자리배정을 하지 않은 것이 제 실수라고 할 수도 있지만, 몇 시간씩 비운 것도 아니고 짐이 있는데 꼭 치우고 앉아야 하셨는지 모르겠습니다. 다행히 다른 자리가 비어 그냥 옮겼지만 기분이 좋지 않네요. 다른 분들도 식사하러 가실 때 꼭 자리배정이 되어 있는지 확인하고 가셔야 이런 일이 없을 것 같습니다.\n",
      "^^^^^^^^^^^^^^^^^\n",
      "0.3268247231186533\n",
      "중앙 도서관이 새롭게 리모델링되었어요!!\n",
      "1층에 열람실 자리표 뽑는 기계도 설치되었고, 배정시간도 4시간으로 넉넉한데\n",
      "우리 모두 열람실 자리표를 뽑아서 쓰는건 어떨까요 &gt;&lt;\n",
      "^^^^^^^^^^^^^^^^^\n",
      "0.30822540305830753\n",
      "시험기간이라 그런지 요즘 카대전에\n",
      "자리 예약안하고 자리 차지하시는 분들에 대해서\n",
      "글이 자주 올라오는 것 같아요.\n",
      "저도 그런 상황을 겪게 된다면 너무 짜증날 것 같아서\n",
      "조금이라도 도움이 되고자 이렇게 글을 씁니다.\n",
      "카이스트 도서관 앱을 받으시면 휴대폰으로 자리를 예약할 수 있는데\n",
      "그러면 자리가 비었는지 일일이 확인할 필요없이\n",
      "빈 자리에 앉은 뒤에 예약을 하실 수 있어요.\n",
      "일일이 자리 확인 안 해도 되어서 정말 편해요.\n",
      "공부하기도 바쁜데 자리 문제 조금이라도 원만하게\n",
      "해결하셨으면 해서 글 올려봅니다.\n",
      "^^^^^^^^^^^^^^^^^\n",
      "0.30736355307929253\n",
      "중도 자리 예약좀 하고 씁시다 ㅡㅡ 한방향 열람실은 자리가 많이 비어도 개별열람실은 꽉 찬단 말이야 ㅡㅡ;; 자리가 많든 적든 제발 예약은 하고 써요 진짜\n",
      "^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "print('Org post-----')\n",
    "print(full_list[search_idx][1].text)\n",
    "print('Org post over-----')\n",
    "for p_idx, d in sorted(sim_info, key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(d)\n",
    "    print(full_list[p_idx][2].body)\n",
    "    print('^^^^^^^^^^^^^^^^^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18570/18570 [00:27<00:00, 685.80it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count_mat = np.zeros((num_docs, num_terms))\n",
    "for p_idx, p, pos_p in tqdm(full_list):\n",
    "    post_dict = tf_by_post[p_idx]\n",
    "    for t_idx, term in enumerate(all_used_terms):\n",
    "        word_count_mat[p_idx][t_idx] = post_dict[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=100, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=100, n_jobs=-1,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(word_count_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "댓글\n",
      "드리다\n",
      "달다\n",
      "남기다\n",
      "가지\n",
      "필요하다\n",
      "페메\n",
      "갖다\n",
      "파다\n",
      "계시\n",
      "어\n",
      "은동\n",
      "궁동\n",
      "사\n",
      "색\n",
      "메세지\n",
      "필통\n",
      "펜\n",
      "노란색\n",
      "섞이다\n",
      "스티커\n",
      "퍼센트\n",
      "계산기\n",
      "가나\n",
      "기프티콘\n",
      "주우\n",
      "수영장\n",
      "샤프\n",
      "두운\n",
      "맛집\n",
      "전자회로\n",
      "뚜레쥬르\n",
      "펜슬\n",
      "지퍼\n",
      "볼펜\n"
     ]
    }
   ],
   "source": [
    "topic_idx = 12\n",
    "topic_vec = lda.components_[topic_idx]\n",
    "max_associated_topic = np.argmax(lda.components_, axis=0)\n",
    "for t_idx, term in enumerate(all_used_terms):\n",
    "    if max_associated_topic[t_idx] == topic_idx:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
